# Prompt-Engineering-LLM
Post articles, research paper and code related to Prompt Engineering and LLM.

<p align="center"><img width="600" src="https://www.xiaoluboke.com/wp-content/uploads/2023/03/prompt-engineering.jpg">

## Ashish Patel Linkedin Post

### [DAY 1](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-artificialintelligence-activity-7043442871990951937-NebM?utm_source=share&utm_medium=member_desktop)

PromptEngineering Learning Google Research has Introduced the Finetuned Langauge models are zero shot learners(FLAN).

ğŸ“ƒ ResearchPaper : https://lnkd.in/dXUxxHyj

ğŸ§‘â€ğŸ’» Code : https://lnkd.in/d4CBWpZH

ğŸš€ INTERESTING FACTS:
------------------------
ğŸ”¸ The paper explores a simple method for improving the zero-shot learning abilities of language models by instruction tuning.

ğŸ”¸ The instruction-tuned model, FLAN, outperforms zero-shot 175B GPT-3 on 20 of 25 evaluated datasets and even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ.

ğŸ”¸ The authors hope that their paper will spur further research on instructions-based NLP, zero-shot learning, and using labeled data to improve large language models.

ğŸ•µï¸â€â™€ï¸ IMPORTANCE:
------------------------
ğŸ”¹ The paper presents a simple yet effective method for improving the zero-shot learning abilities of language models.

ğŸ”¹ The instruction tuning approach used in the paper can help language models to perform better on unseen tasks without requiring any additional training data.

ğŸ”¹ This approach can reduce the need for expensive and time-consuming data labeling efforts, making it more cost-effective and efficient.

ğŸ”¹ The results of the paper show that FLAN, the instruction-tuned model, outperforms zero-shot 175B GPT-3 on 20 of 25 evaluated datasets and even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ.

ğŸ”¹ This suggests that instruction tuning has great potential to improve the performance of large language models at scale.

ğŸ”¹ The paper's findings could have significant implications for natural language processing (NLP) applications such as chatbots, virtual assistants, and machine translation systems.

ğŸ”¹ The authors hope that their work will spur further research on instructions-based NLP, zero-shot learning, and using labeled data to improve large language models.

<p align="center"><img width="600" src="https://media.licdn.com/dms/image/C4D22AQHCOyK8lPMjtQ/feedshare-shrink_800/0/1679287639984?e=1684368000&v=beta&t=OcB_S7X4hUpbwozJp4rzDnVwBjQ0_h387dfH8Qa6z3g">



### [DAY 2](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-artificialintelligence-activity-7044527071057850368-Wf_I?utm_source=share&utm_medium=member_desktop)

### [DAY 3](https://www.linkedin.com/posts/ashishpatel2604_ai-artificialintelligence-machinelearning-activity-7046347361631055872-vJVW?utm_source=share&utm_medium=member_desktop)

### [DAY 4](https://www.linkedin.com/posts/ashishpatel2604_innovation-artificialintelligence-promptengineering-activity-7046711374248390656-856P?utm_source=share&utm_medium=member_desktop)

### [DAY 5](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-machinelearning-activity-7047431337971990528-Th_D?utm_source=share&utm_medium=member_desktop)

### [DAY 6](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-chatgpt-gpt4-activity-7048880973668659200-vA8Q?utm_source=share&utm_medium=member_desktop)

### [DAY 7](https://www.linkedin.com/posts/ashishpatel2604_16-prompt-techniques-used-in-paper-activity-7049245926938329089-OP2Y?utm_source=share&utm_medium=member_desktop)

### [DAY 8](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-artificialintelligence-activity-7051407644409905152-SfAZ?utm_source=share&utm_medium=member_desktop)



## Credits
- [Ashish Patel](https://github.com/ashishpatel26)
- [Prompt Engineering image](https://www.xiaoluboke.com/wp-content/uploads/2023/03/prompt-engineering.jpg)
