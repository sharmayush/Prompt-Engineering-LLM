# Prompt-Engineering-LLM
Post articles, research paper and code related to Prompt Engineering and LLM.

<p align="center"><img width="600" src="https://www.xiaoluboke.com/wp-content/uploads/2023/03/prompt-engineering.jpg">

## Ashish Patel Linkedin Post

### [DAY 1](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-artificialintelligence-activity-7043442871990951937-NebM?utm_source=share&utm_medium=member_desktop)

PromptEngineering Learning Google Research has Introduced the Finetuned Langauge models are zero shot learners(FLAN).

ğŸ“ƒ ResearchPaper : https://lnkd.in/dXUxxHyj

ğŸ§‘â€ğŸ’» Code : https://lnkd.in/d4CBWpZH

ğŸš€ INTERESTING FACTS:
------------------------
ğŸ”¸ The paper explores a simple method for improving the zero-shot learning abilities of language models by instruction tuning.

ğŸ”¸ The instruction-tuned model, FLAN, outperforms zero-shot 175B GPT-3 on 20 of 25 evaluated datasets and even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ.

ğŸ”¸ The authors hope that their paper will spur further research on instructions-based NLP, zero-shot learning, and using labeled data to improve large language models.

ğŸ•µï¸â€â™€ï¸ IMPORTANCE:
------------------------
ğŸ”¹ The paper presents a simple yet effective method for improving the zero-shot learning abilities of language models.

ğŸ”¹ The instruction tuning approach used in the paper can help language models to perform better on unseen tasks without requiring any additional training data.

ğŸ”¹ This approach can reduce the need for expensive and time-consuming data labeling efforts, making it more cost-effective and efficient.

ğŸ”¹ The results of the paper show that FLAN, the instruction-tuned model, outperforms zero-shot 175B GPT-3 on 20 of 25 evaluated datasets and even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ.

ğŸ”¹ This suggests that instruction tuning has great potential to improve the performance of large language models at scale.

ğŸ”¹ The paper's findings could have significant implications for natural language processing (NLP) applications such as chatbots, virtual assistants, and machine translation systems.

ğŸ”¹ The authors hope that their work will spur further research on instructions-based NLP, zero-shot learning, and using labeled data to improve large language models.

<p align="center"><img width="600" src="https://media.licdn.com/dms/image/C4D22AQHCOyK8lPMjtQ/feedshare-shrink_800/0/1679287639984?e=1684368000&v=beta&t=OcB_S7X4hUpbwozJp4rzDnVwBjQ0_h387dfH8Qa6z3g">



### [DAY 2](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-artificialintelligence-activity-7044527071057850368-Wf_I?utm_source=share&utm_medium=member_desktop)

PromptEngineering Language Models as Few-Shot Learners

ğŸ“ƒ ResearchPaper : https://lnkd.in/dUYcMSkB

ğŸ§‘â€ğŸ’» Code : https://lnkd.in/drQWySc3

ğŸš€ INTERESTING FACTS:
------------------------
ğŸ”¸ The paper introduces the concept of few-shot learning in NLP tasks, which is the ability of a language model to perform a new task from only a few examples or simple instructions.

ğŸ”¸ The authors use GPT-3 175B, one of the largest language models available at the time of writing, to demonstrate the effectiveness of few-shot learning on a variety of NLP tasks.

ğŸ”¸ The paper includes detailed descriptions and examples of various NLP tasks used to evaluate the performance of language models in zero-shot, one-shot, and few-shot settings.

ğŸ•µï¸â€â™€ï¸ IMPORTANCE:
------------------------
ğŸ”¹ The paper introduces the concept of few-shot learning in NLP tasks, which is a significant advancement in the field of natural language processing.

ğŸ”¹ The authors demonstrate that language models can perform well on new tasks with only a few examples or simple instructions, which has important implications for real-world applications.

ğŸ”¹ The paper evaluates the performance of GPT-3 175B, one of the largest language models available at the time of writing, on a variety of NLP tasks, providing valuable insights into its capabilities and limitations.

ğŸ”¹ The authors compare the performance of GPT-3 to other state-of-the-art language models and show that it outperforms them on many tasks.

ğŸ”¹ The paper includes detailed descriptions and examples of various NLP tasks used to evaluate the performance of language models in zero-shot, one-shot, and few-shot settings, providing a comprehensive overview of current research in this area.

ğŸ”¹ The authors discuss potential applications for few-shot learning in areas such as chatbots, question answering systems, and machine translation.

ğŸ”¹ The paper highlights the importance of pre-training on large corpora of text followed by fine-tuning on specific tasks for achieving high performance on NLP benchmarks.

ğŸ”¹ Overall, this paper represents an important contribution to the field of natural language processing and has significant implications for future research and development in this area.


<p align="center"><img width="600" src="https://media.licdn.com/dms/image/D4D22AQHtW8TS54uz7g/feedshare-shrink_800/0/1679546133698?e=1684368000&v=beta&t=i13qsYnm7FW5XZ-ZtYaixX79-83htUr-5zNmCtHZaHw">


### [DAY 3](https://www.linkedin.com/posts/ashishpatel2604_ai-artificialintelligence-machinelearning-activity-7046347361631055872-vJVW?utm_source=share&utm_medium=member_desktop)

Prompt Engineering: Google #AI Research Finds Chain-of-Thought Prompting Boosts Reasoning in Large Language Models

ğŸ“ƒ Paper : https://lnkd.in/dC7Td7Hy

ğŸ§‘â€ğŸ’» Code : https://lnkd.in/d-JusXSF

ğŸ•µï¸â€â™€ï¸ Abstract:

ğŸ”¸ The paper explores how generating a chain of thought improves the ability of large language models to perform complex reasoning.

ğŸ”¸ Chain-of-thought prompting is a simple method that demonstrates how reasoning abilities emerge naturally in sufficiently large language models.

ğŸ›« Introduction:

ğŸ”¸ The paper discusses the limitations of current language models in performing complex reasoning tasks.

ğŸ”¸ It introduces chain-of-thought prompting as a solution to this problem.

ğŸ”¸ The paper outlines the structure of the rest of the paper.

ğŸ“ƒ Related Work:

ğŸ”¸ The paper discusses previous research on few-shot learning and prompting in language models.

ğŸ”¸ It highlights the differences between these approaches and chain-of-thought prompting.

âš™ï¸ Methodology:

ğŸ”¸ The paper describes how chain-of-thought prompts are constructed and used to train language models.

ğŸ”¸ It explains how three large language models were trained using this method.

ğŸ§‘â€ğŸ”¬ Experiments:

ğŸ”¸ The paper presents results from experiments conducted on three large language models using various reasoning tasks.

ğŸ”¸ It compares the performance of these models with and without chain-of-thought prompting.

ğŸ¤ Discussion:

ğŸ”¸ The paper discusses the implications of its findings for natural language processing and artificial intelligence research.

ğŸ”¸ It also highlights some limitations and potential future directions for this research.

ğŸš€ List of concepts used in the Paper:
------
âœ… Chain-of-thought prompting \
âœ… Large language models \
âœ… Complex reasoning tasks \
âœ… Few-shot learning \
âœ… Prompting \
âœ… Reasoning abilities \
âœ… Model scale \
âœ… Arithmetic reasoning \
âœ… Symbolic reasoning \
âœ… Commonsense reasoning

<p align="center"><img width="600" src="https://media.licdn.com/dms/image/D4D22AQEq9bdLJ4QidQ/feedshare-shrink_800/0/1679980123906?e=1684368000&v=beta&t=jz5TGEZdHVUUIRSE4jF2uAHCqpH_J013Z-ni-Uwl_AE">


### [DAY 4](https://www.linkedin.com/posts/ashishpatel2604_innovation-artificialintelligence-promptengineering-activity-7046711374248390656-856P?utm_source=share&utm_medium=member_desktop)

### [DAY 5](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-machinelearning-activity-7047431337971990528-Th_D?utm_source=share&utm_medium=member_desktop)

### [DAY 6](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-chatgpt-gpt4-activity-7048880973668659200-vA8Q?utm_source=share&utm_medium=member_desktop)

### [DAY 7](https://www.linkedin.com/posts/ashishpatel2604_16-prompt-techniques-used-in-paper-activity-7049245926938329089-OP2Y?utm_source=share&utm_medium=member_desktop)

### [DAY 8](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-artificialintelligence-activity-7051407644409905152-SfAZ?utm_source=share&utm_medium=member_desktop)



## Credits
- [Ashish Patel](https://github.com/ashishpatel26)
- [Prompt Engineering image](https://www.xiaoluboke.com/wp-content/uploads/2023/03/prompt-engineering.jpg)
