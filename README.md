# Prompt-Engineering-LLM
Post articles, research paper and code related to Prompt Engineering and LLM.

<p align="center"><img width="600" src="https://www.xiaoluboke.com/wp-content/uploads/2023/03/prompt-engineering.jpg">

## Ashish Patel Linkedin Post

### [DAY 1](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-artificialintelligence-activity-7043442871990951937-NebM?utm_source=share&utm_medium=member_desktop)

PromptEngineering Learning Google Research has Introduced the Finetuned Langauge models are zero shot learners(FLAN).

📃 ResearchPaper : https://lnkd.in/dXUxxHyj

🧑‍💻 Code : https://lnkd.in/d4CBWpZH

🚀 INTERESTING FACTS:
------------------------
🔸 The paper explores a simple method for improving the zero-shot learning abilities of language models by instruction tuning.

🔸 The instruction-tuned model, FLAN, outperforms zero-shot 175B GPT-3 on 20 of 25 evaluated datasets and even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ.

🔸 The authors hope that their paper will spur further research on instructions-based NLP, zero-shot learning, and using labeled data to improve large language models.

🕵️‍♀️ IMPORTANCE:
------------------------
🔹 The paper presents a simple yet effective method for improving the zero-shot learning abilities of language models.

🔹 The instruction tuning approach used in the paper can help language models to perform better on unseen tasks without requiring any additional training data.

🔹 This approach can reduce the need for expensive and time-consuming data labeling efforts, making it more cost-effective and efficient.

🔹 The results of the paper show that FLAN, the instruction-tuned model, outperforms zero-shot 175B GPT-3 on 20 of 25 evaluated datasets and even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ.

🔹 This suggests that instruction tuning has great potential to improve the performance of large language models at scale.

🔹 The paper's findings could have significant implications for natural language processing (NLP) applications such as chatbots, virtual assistants, and machine translation systems.

🔹 The authors hope that their work will spur further research on instructions-based NLP, zero-shot learning, and using labeled data to improve large language models.

<p align="center"><img width="600" src="https://media.licdn.com/dms/image/C4D22AQHCOyK8lPMjtQ/feedshare-shrink_800/0/1679287639984?e=1684368000&v=beta&t=OcB_S7X4hUpbwozJp4rzDnVwBjQ0_h387dfH8Qa6z3g">



### [DAY 2](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-artificialintelligence-activity-7044527071057850368-Wf_I?utm_source=share&utm_medium=member_desktop)

PromptEngineering Language Models as Few-Shot Learners

📃 ResearchPaper : https://lnkd.in/dUYcMSkB

🧑‍💻 Code : https://lnkd.in/drQWySc3

🚀 INTERESTING FACTS:
------------------------
🔸 The paper introduces the concept of few-shot learning in NLP tasks, which is the ability of a language model to perform a new task from only a few examples or simple instructions.

🔸 The authors use GPT-3 175B, one of the largest language models available at the time of writing, to demonstrate the effectiveness of few-shot learning on a variety of NLP tasks.

🔸 The paper includes detailed descriptions and examples of various NLP tasks used to evaluate the performance of language models in zero-shot, one-shot, and few-shot settings.

🕵️‍♀️ IMPORTANCE:
------------------------
🔹 The paper introduces the concept of few-shot learning in NLP tasks, which is a significant advancement in the field of natural language processing.

🔹 The authors demonstrate that language models can perform well on new tasks with only a few examples or simple instructions, which has important implications for real-world applications.

🔹 The paper evaluates the performance of GPT-3 175B, one of the largest language models available at the time of writing, on a variety of NLP tasks, providing valuable insights into its capabilities and limitations.

🔹 The authors compare the performance of GPT-3 to other state-of-the-art language models and show that it outperforms them on many tasks.

🔹 The paper includes detailed descriptions and examples of various NLP tasks used to evaluate the performance of language models in zero-shot, one-shot, and few-shot settings, providing a comprehensive overview of current research in this area.

🔹 The authors discuss potential applications for few-shot learning in areas such as chatbots, question answering systems, and machine translation.

🔹 The paper highlights the importance of pre-training on large corpora of text followed by fine-tuning on specific tasks for achieving high performance on NLP benchmarks.

🔹 Overall, this paper represents an important contribution to the field of natural language processing and has significant implications for future research and development in this area.


<p align="center"><img width="600" src="https://media.licdn.com/dms/image/D4D22AQHtW8TS54uz7g/feedshare-shrink_800/0/1679546133698?e=1684368000&v=beta&t=i13qsYnm7FW5XZ-ZtYaixX79-83htUr-5zNmCtHZaHw">


### [DAY 3](https://www.linkedin.com/posts/ashishpatel2604_ai-artificialintelligence-machinelearning-activity-7046347361631055872-vJVW?utm_source=share&utm_medium=member_desktop)

Prompt Engineering: Google #AI Research Finds Chain-of-Thought Prompting Boosts Reasoning in Large Language Models

📃 Paper : https://lnkd.in/dC7Td7Hy

🧑‍💻 Code : https://lnkd.in/d-JusXSF

🕵️‍♀️ Abstract:

🔸 The paper explores how generating a chain of thought improves the ability of large language models to perform complex reasoning.

🔸 Chain-of-thought prompting is a simple method that demonstrates how reasoning abilities emerge naturally in sufficiently large language models.

🛫 Introduction:

🔸 The paper discusses the limitations of current language models in performing complex reasoning tasks.

🔸 It introduces chain-of-thought prompting as a solution to this problem.

🔸 The paper outlines the structure of the rest of the paper.

📃 Related Work:

🔸 The paper discusses previous research on few-shot learning and prompting in language models.

🔸 It highlights the differences between these approaches and chain-of-thought prompting.

⚙️ Methodology:

🔸 The paper describes how chain-of-thought prompts are constructed and used to train language models.

🔸 It explains how three large language models were trained using this method.

🧑‍🔬 Experiments:

🔸 The paper presents results from experiments conducted on three large language models using various reasoning tasks.

🔸 It compares the performance of these models with and without chain-of-thought prompting.

🤝 Discussion:

🔸 The paper discusses the implications of its findings for natural language processing and artificial intelligence research.

🔸 It also highlights some limitations and potential future directions for this research.

🚀 List of concepts used in the Paper:
------
✅ Chain-of-thought prompting \
✅ Large language models \
✅ Complex reasoning tasks \
✅ Few-shot learning \
✅ Prompting \
✅ Reasoning abilities \
✅ Model scale \
✅ Arithmetic reasoning \
✅ Symbolic reasoning \
✅ Commonsense reasoning

<p align="center"><img width="600" src="https://media.licdn.com/dms/image/D4D22AQEq9bdLJ4QidQ/feedshare-shrink_800/0/1679980123906?e=1684368000&v=beta&t=jz5TGEZdHVUUIRSE4jF2uAHCqpH_J013Z-ni-Uwl_AE">


### [DAY 4](https://www.linkedin.com/posts/ashishpatel2604_innovation-artificialintelligence-promptengineering-activity-7046711374248390656-856P?utm_source=share&utm_medium=member_desktop)

### [DAY 5](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-machinelearning-activity-7047431337971990528-Th_D?utm_source=share&utm_medium=member_desktop)

### [DAY 6](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-chatgpt-gpt4-activity-7048880973668659200-vA8Q?utm_source=share&utm_medium=member_desktop)

### [DAY 7](https://www.linkedin.com/posts/ashishpatel2604_16-prompt-techniques-used-in-paper-activity-7049245926938329089-OP2Y?utm_source=share&utm_medium=member_desktop)

### [DAY 8](https://www.linkedin.com/posts/ashishpatel2604_promptengineering-largelanguagemodels-artificialintelligence-activity-7051407644409905152-SfAZ?utm_source=share&utm_medium=member_desktop)



## Credits
- [Ashish Patel](https://github.com/ashishpatel26)
- [Prompt Engineering image](https://www.xiaoluboke.com/wp-content/uploads/2023/03/prompt-engineering.jpg)
